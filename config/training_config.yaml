# Configuration for Rainbow DQN Trading Agent Training

window_size: 60

logging:
  log_filename: training.log
  root_level: INFO
  console_level: INFO
  file_level: INFO
  level_overrides:
    Trainer: INFO
    Agent: INFO
    DataManager: INFO
    TransformerModel: INFO
    Buffer: INFO
    Metrics: INFO
    Evaluation: INFO

agent:
  # Core RL params
  gamma: 0.99
  lr: 0.001 # Learning rate (reduced for stability with transformer backbone)
  batch_size: 4096
  replay_buffer_size: 500000
  target_update_freq: 600 # Learner steps per target sync (~1-2 trading days)

  # Network architecture
  window_size: 60 # Window size for the agent same as the environment
  n_features: 5
  hidden_dim: 256
  num_actions: 7 # Needs to match environment action space

  # Transformer specific params (add if missing)
  nhead: 4 # Number of attention heads
  num_encoder_layers: 3 # Example: Number of Transformer encoder layers
  dim_feedforward: 512 # Example: Feedforward dimension
  transformer_dropout: 0.2 # Example: Dropout within transformer

  # Rainbow specific params
  n_steps: 5          # Multi-step returns
  num_atoms: 51       # Distributional RL atoms
  v_min: -10.0       # Distributional RL support min value (wider to cover scaled rewards)
  v_max: 10.0        # Distributional RL support max value (wider to cover scaled rewards)
  alpha: 0.6          # PER priority exponent
  beta_start: 0.3     # PER importance sampling exponent (initial)
  beta_frames: 400000 # PER beta annealing frames (steps)

  # Learning rate scheduler params
  lr_scheduler_enabled: true # Enable/disable LR scheduler
  lr_scheduler_type: 'ReduceLROnPlateau' # e.g., 'StepLR', 'CosineAnnealingLR', 'ReduceLROnPlateau'
  lr_scheduler_params: # Parameters specific to the chosen scheduler
    mode: 'max'        # 'max' because higher validation score is better
    factor: 0.5        # Factor by which the learning rate will be reduced. new_lr = lr * factor
    patience: 10       # Number of epochs with no improvement after which learning rate will be reduced
    threshold: 0.0001  # Threshold for measuring the new optimum, to only focus on significant changes
    min_lr: 0.000001   # A lower bound on the learning rate

  # Other agent params
  grad_clip_norm: 1.0 # Gradient clipping norm
  debug: False        # Enable debug checks (e.g., gradient checks)

environment:
  window_size: 60 # Window size for the environment same as the agent
  initial_balance: 1000.0
  transaction_fee: 0.001 # Percentage fee
  reward_scale: 50.0  # Scale factor for PnL component of reward
  invalid_action_penalty: -0.05 # Penalty for invalid actions
  render_mode:  # Set to 'human' or 'terminal' to enable visualization

trainer:
  seed: 42
  warmup_steps: 30000     # Steps with random actions before training
  update_freq: 10          # Agent learning update frequency (steps)
  gradient_updates_per_step: 1  # Number of learner updates to run when update condition is met; increase to better utilize GPU
  log_freq: 60            # Logging frequency (steps)
  per_stats_log_freq: 120 # Frequency (env steps) for PER stats between episode summaries; set to 0 for episode-only logs
  validation_freq: 45     # Validation frequency (episodes)
  checkpoint_save_freq: 30 # Checkpoint save frequency (episodes)
  reward_window: 10       # Window for averaging episode rewards in logs
  early_stopping_patience: 15 # Episodes without validation improvement to stop
  min_validation_threshold: 0.0 # Minimum score change considered improvement
  render_training: true # Enable live rendering during training episodes
  render_training_every_n_steps: 10 # Render frequency (in environment steps) when enabled
  render_validation: false # Enable live rendering during validation episodes
  render_validation_every_n_steps: 1 # Render frequency during validation when enabled
  render_evaluation: false # Enable live rendering during final evaluation runs
  render_evaluation_every_n_steps: 1 # Render frequency during evaluation when enabled
  render_on_reset: false # Render immediately after each env.reset() when rendering is enabled

# --- Run Configuration --- #
run:
  mode: 'train'              # 'train' or 'eval'
  episodes: 50000            # Number of training episodes
  model_dir: 'models'        # Directory to save models and logs
  resume: false             # Resume training from latest checkpoint if true
  specific_file: null       # Path to a specific training file (relative to data root) or null/None
  skip_evaluation: false    # Skip final evaluation on test set after training
  # data_base_dir: 'data'   # Optional: Override base data directory (defaults to 'data')
  # eval_model_prefix: 'models/rainbow_transformer_best' # Optional: Model prefix for eval mode
